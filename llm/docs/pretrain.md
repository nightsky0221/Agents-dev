# å¤§æ¨¡å‹é¢„è®­ç»ƒä»‹ç»

PaddleNLP å¤§æ¨¡å‹å¥—ä»¶æ”¯æŒå¤šç§å¤§æ¨¡å‹çš„é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬ä½†ä¸é™äº LLaMA v1/v2ã€GPT-3ã€BaiChuan å’Œ Qwen ç­‰,è¿™äº›æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œè¿™äº›æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œæ¯”å¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚å³ä½¿æ˜¯æ–°æ‰‹å°ç™½ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ç®€å•çš„æ­¥éª¤**å¿«é€Ÿä¸Šæ‰‹**å¤§æ¨¡å‹çš„é¢„è®­ç»ƒã€‚



## å‡†å¤‡å·¥ä½œ

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦å®‰è£… PaddleNLP çš„æœ€æ–°ç‰ˆæœ¬ï¼Œè¿™é‡Œæ¨èå®‰è£… `develop` ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒåŒ…å«äº†æœ€æ–°çš„åŠŸèƒ½å’Œæ”¹è¿›ã€‚

```bash
pip install --pre --upgrade paddlenlp -f https://www.paddlepaddle.org.cn/whl/paddlenlp.html

# å¦‚æœä¸‹è½½é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨å›½å†…é•œåƒï¼Œå¦‚ç™¾åº¦é•œåƒï¼š
pip install --pre --upgrade paddlenlp -f https://mirror.baidu.com/paddlepaddle/whl/paddlenlp.html
```



æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦å°† PaddleNLP çš„ä»£ç å…‹éš†åˆ°æœ¬åœ°ï¼š

```bash
# æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å…‹éš† PaddleNLP ä»£ç åˆ°æœ¬åœ°ï¼š
git clone https://github.com/PaddlePaddle/PaddleNLP.git

# å¦‚æœå…‹éš†é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨ Gitee é•œåƒè¿›è¡Œå…‹éš†ï¼š
# æ³¨æ„ï¼šgiteeåŒæ­¥æ—¶é—´ä¸åŒï¼Œå¯èƒ½å‡ºç°æ»å
git clone https://gitee.com/PaddlePaddle/PaddleNLP.git

# å…‹éš†å®Œæˆåï¼Œè¿›å…¥ llm ç›®å½•ï¼Œè¿™æ˜¯è¿è¡Œå¤§æ¨¡å‹é¢„è®­ç»ƒçš„ç›®å½•
cd PaddleNLP/llm
```



## æ•°æ®åˆ¶ä½œ

åœ¨å¼€å§‹é¢„è®­ç»ƒä¹‹å‰ï¼Œæ‚¨éœ€è¦å‡†å¤‡è®­ç»ƒæ•°æ®ã€‚PaddleNLP æä¾›äº†å¤šç§å†…ç½®æ•°æ®é›†ï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰æ•°æ®çš„åˆ¶ä½œï¼Œæ‚¨å¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡æ¡£æ¥å‡†å¤‡æ•°æ®ï¼š

- [å†…ç½®é¢„ç»ƒæ•°æ®é›†åŠè‡ªå®šä¹‰æ•°æ®åˆ¶ä½œ](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/zh/llm/dataset.md)

- [CLUECorpus2020 è¯­æ–™åˆ¶ä½œ](../tools/preprocess/docs/CLUECorpus2020.md)
- [CLUECorpusSmall è¯­æ–™åˆ¶ä½œ](../tools/preprocess/docs/CLUECorpusSmall.md)
- [OpenWebText2 è¯­æ–™åˆ¶ä½œ](../tools/preprocess/docs/OpenWebText2.md)
- [WuDaoCorpus2.0 Base è¯­æ–™](../tools/preprocess/docs/WuDaoCorpusBase.md)



## å¼€å§‹è®­ç»ƒ

ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·è¿è¡Œæµ‹è¯•æœ¬æ¨¡å‹ï¼Œæœ¬é¡¹ç›®æä¾›äº†å¤„ç†å¥½çš„100k æ¡ doc çš„è®­ç»ƒæ ·æœ¬ï¼š

```bash
# llama æ¨¡å‹æ•°æ®ä¸‹è½½
wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k.bin
wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k.idx

# gpt æ¨¡å‹æ•°æ®ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
# wget https://bj.bcebos.com/paddlenlp/models/transformers/gpt/data/gpt_en_dataset_300m_ids.npy
# wget https://bj.bcebos.com/paddlenlp/models/transformers/gpt/data/gpt_en_dataset_300m_idx.npz
```

å°†æ‰€æœ‰é¢„å¤„ç†å¾—åˆ°çš„æ–‡ä»¶ç»Ÿä¸€æ”¾å…¥ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å¤‡è®­ç»ƒä½¿ç”¨ï¼š

```bash
# åˆ›å»º data ç›®å½•
mkdir data

# ç§»åŠ¨æ–‡ä»¶è‡³ data ç›®å½•ä¸‹
mv llama_openwebtext_100k.bin ./data
mv llama_openwebtext_100k.idx ./data
```

ç¼–è¯‘è‡ªå®šä¹‰ç®—å­ï¼ˆå¯é€‰ï¼‰ï¼š

```bash
cd ../slm/model_zoo/gpt-3/external_ops/
python3 setup.py install
cd -
```

è¿è¡Œé¢„è®­ç»ƒå‘½ä»¤ï¼š

```bash
# llama æ¨¡å‹é¢„è®­ç»ƒ
python -u -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" run_pretrain.py ./config/llama/pretrain_argument.json

# Qwen æ¨¡å‹é¢„è®­ç»ƒ
python -u -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" run_pretrain.py ./config/qwen/pretrain_argument.json
```

é¢„è®­ç»ƒæˆåŠŸåï¼Œæ‰“å°ä¿¡æ¯å¦‚ä¸‹ï¼ˆä»¥ Qwen ä¸ºä¾‹ï¼‰ï¼š

```bash
# æœ€ç»ˆçš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®
Final pre-training config: Qwen2Config {
  # æ¨¡å‹æ¶æ„ï¼šQwen2
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  ...
  "vocab_size": 152064
}

# ä¸‹è½½è¿›åº¦ä¸º 25%ï¼Œå½“å‰æ­£åœ¨ä¸‹è½½ç¬¬ 1 ä¸ªåˆ†ç‰‡ï¼Œæ€»å…±æœ‰ 4 ä¸ªåˆ†ç‰‡ï¼›100%å³ä»£è¡¨åˆ†ç‰‡ä¸‹è½½å®Œæˆ
Downloading shards:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 1/4 [00:43<02:11, 43.73s/it]
Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                          | 2/4 [01:27<01:27, 43.84s/it]
Downloading shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 3/4 [02:10<01:05, 43.92s/it]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 4/4 [03:15<00:00, 43.95s/it]
```



## æ³¨æ„äº‹é¡¹

1. å»ºè®®ä½¿ç”¨ Paddle develop ç‰ˆæœ¬è®­ç»ƒï¼Œéœ€è¦å®‰è£…`pip install fast_dataindex visualdl==2.5.3`ç­‰ç›¸å…³ç¼ºå¤± whl åŒ…ã€‚
2. `use_flash_attention`éœ€åœ¨ A100æœºå™¨å¼€å¯ï¼Œå½“å‰æ”¯æŒçš„ cuda ç‰ˆæœ¬æœ€ä½æ˜¯11.8ï¼Œä¸è¿‡æœ€æ¨èçš„æ˜¯å®˜ç½‘æœ€æ–° cuda ç‰ˆæœ¬ã€‚
3. `use_fused_rms_norm`éœ€è¦å®‰è£…[è‡ªå®šä¹‰ OP](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/slm/model_zoo/gpt-3/external_ops)ã€‚å¦‚æœå®‰è£…åä»ç„¶æ‰¾ä¸åˆ°ç®—å­ï¼Œéœ€è¦é¢å¤–è®¾ç½®`PYTHONPATH`ã€‚
4. `continue_training`è¡¨ç¤ºä»ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½è®­ç»ƒã€‚7B æ¨¡å‹åˆå§‹ loss å¤§æ¦‚ä¸º2.xxï¼Œéšæœºåˆå§‹åŒ–æ¨¡å‹ loss ä»11.x å·¦å³ä¸‹é™ã€‚
5. å½“å‰è„šæœ¬ä¸º Sharding ç‰ˆæœ¬ï¼Œéœ€è¦4D å¹¶è¡Œè®­ç»ƒï¼ˆæ•°æ®ã€Shardingã€å¼ é‡ã€æµæ°´çº¿å¹¶è¡Œï¼‰çš„ç”¨æˆ·ï¼Œè¯·å‚è€ƒ [run_trainer_tp4pp2.sh](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/experimental/scripts/run_trainer_tp4pp2.sh) è„šæœ¬ã€‚
6. å¤šæœºè®­ç»ƒæ—¶ï¼Œè‹¥å„æœºå™¨ä½¿ç”¨çš„è®­ç»ƒæ•°æ®æ–‡ä»¶ä½ç½®ç›¸åŒï¼ˆä¾‹å¦‚æŒ‚è½½å…±äº«ç¡¬ç›˜æƒ…å†µï¼‰ï¼Œè¯·æŒ‡å®š`--share_folder true`ä½¿å…¨å±€0å·å¡åˆ¶ä½œç¼“å­˜æ•°æ®ã€‚å¦åˆ™é»˜è®¤å„å°æœºå™¨çš„0å·å¡ç‹¬ç«‹åˆ¶ä½œç¼“å­˜æ•°æ®ã€‚
7. è‹¥æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­å­˜åœ¨é»˜è®¤ç¼“å­˜æ–‡ä»¶å¤¹`index-cache/`ï¼Œåˆ™é¢å¤–æŒ‡å®šçš„`--data_cache`ä¸ç”Ÿæ•ˆï¼Œè®­ç»ƒæ—¶ä¼˜å…ˆåŠ è½½é»˜è®¤ç¼“å­˜æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹ã€‚

é¢„è®­ç»ƒä½¿ç”¨äº† PaddleNLP çš„ Trainer æ¨¡å—ï¼Œç›¸å…³åˆ†å¸ƒå¼ç­–ç•¥ä½¿ç”¨ï¼Œè¯·å‚è€ƒ[å¤§æ¨¡å‹ Trainer æ··åˆå¹¶è¡Œè®­ç»ƒæ•™ç¨‹](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/zh/trainer.md)ã€‚



## æ¨¡å‹é¢„è®­ç»ƒæ”¯æŒçš„åˆ†å¸ƒå¼èƒ½åŠ›ä¸€è§ˆ

| Model        | Data Parallelism | Tensor Parallelism | Pipeline Parallelism | Sequence Parallelism | Flash Attention | Selective Recompute | Sharding Stage1 + recompute | Sharding Stage1 + DP | Stage2 + recompute | Stage2 + DP | Stage3 + recompute | Stage3 + DP |
| ------------ | ---------------- | ------------------ | -------------------- | -------------------- | --------------- | ------------------- | --------------------------- | -------------------- | ------------------ | ----------- | ------------------ | ----------- |
| LLaMA-65B    | âœ…                | âœ…                  | âœ…                    | âœ…                    | âœ…               | âœ…                   | âœ…                           | âœ…                    | âœ…                  | âœ…           | âœ…                  | âœ…           |
| LLaMA2-70B   | âœ…                | âœ…                  | âœ…                    | âœ…                    | âœ…               | âœ…                   | âœ…                           | âœ…                    | âœ…                  | âœ…           | âœ…                  | âœ…           |
| BaiChuan-13B | âœ…                | âœ…                  | âœ…                    | âœ…                    | âœ…               | âœ…                   | âœ…                           | âœ…                    | âœ…                  | âœ…           | âœ…                  | âœ…           |
| GPT3         | âœ…                | âœ…                  | âœ…                    | âœ…                    | âœ…               | âœ…                   | âœ…                           | âœ…                    | âœ…                  | âœ…           | âœ…                  | âœ…           |
| Qwen-7B      | âœ…                | âœ…                  | âœ…                    | â¬œ                    | âœ…               | âœ…                   | â¬œ                           | âœ…                    | âœ…                  | âœ…           | âœ…                  | âœ…           |
| Qwen-14B     | âœ…                | âœ…                  | âœ…                    | â¬œ                    | âœ…               | âœ…                   | â¬œ                           | âœ…                    | âœ…                  | âœ…           | âœ…                  | âœ…           |
| OPT 66B      | âœ…                | âœ…                  | â¬œ                    | â¬œ                    | âŒ               | ğŸš§                   | â¬œ                           | â¬œ                    | â¬œ                  | â¬œ           | â¬œ                  | â¬œ           |
| Bloom-176B   | âœ…                | âœ…                  | â¬œ                    | â¬œ                    | âœ…               | ğŸš§                   | â¬œ                           | â¬œ                    | â¬œ                  | â¬œ           | â¬œ                  | â¬œ           |
| ChatGLM-6B   | âœ…                | âœ…                  | â¬œ                    | â¬œ                    | âœ…               | ğŸš§                   | â¬œ                           | â¬œ                    | â¬œ                  | â¬œ           | â¬œ                  | â¬œ           |
| ChatGLM2     | âœ…                | âœ…                  | â¬œ                    | â¬œ                    | âŒ               | ğŸš§                   | â¬œ                           | â¬œ                    | â¬œ                  | â¬œ           | â¬œ                  | â¬œ           |
| GLM 130B     | âœ…                | âœ…                  | â¬œ                    | â¬œ                    | âŒ               | ğŸš§                   | â¬œ                           | â¬œ                    | â¬œ                  | â¬œ           | â¬œ                  | â¬œ           |

* âœ…: å·²æ”¯æŒï¼ŒSupported
* ğŸš§: éƒ¨åˆ†æ”¯æŒï¼ŒIn Progress
* âŒ: æš‚ä¸æ”¯æŒï¼ŒNot Supported



## æ¨¡å‹æƒé‡æ”¯æŒåˆ—è¡¨

|                           æ¨¡å‹ç³»åˆ—                           | æ¨¡å‹åç§°                                                     |
| :----------------------------------------------------------: | :----------------------------------------------------------- |
| [PP-UIE](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/application/information_extraction) | paddlenlp/PP-UIE-0.5B, paddlenlp/PP-UIE-1.5B, paddlenlp/PP-UIE-7B, paddlenlp/PP-UIE-14B |
| [LLaMA](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/llama) | facebook/llama-7b, facebook/llama-13b, facebook/llama-30b, facebook/llama-65b |
| [Llama2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/llama) | meta-llama/Llama-2-7b, meta-llama/Llama-2-7b-chat, meta-llama/Llama-2-13b, meta-llama/Llama-2-13b-chat, meta-llama/Llama-2-70b, meta-llama/Llama-2-70b-chat |
| [Llama3](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/llama) | meta-llama/Meta-Llama-3-8B, meta-llama/Meta-Llama-3-8B-Instruct, meta-llama/Meta-Llama-3-70B, meta-llama/Meta-Llama-3-70B-Instruct |
| [Llama3.1](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/llama) | meta-llama/Meta-Llama-3.1-8B, meta-llama/Meta-Llama-3.1-8B-Instruct, meta-llama/Meta-Llama-3.1-70B, meta-llama/Meta-Llama-3.1-70B-Instruct, meta-llama/Meta-Llama-3.1-405B, meta-llama/Meta-Llama-3.1-405B-Instruct, meta-llama/Llama-Guard-3-8B |
| [Llama3.2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/llama) | meta-llama/Llama-3.2-1B, meta-llama/Llama-3.2-1B-Instruct, meta-llama/Llama-3.2-3B, meta-llama/Llama-3.2-3B-Instruct, meta-llama/Llama-Guard-3-1B |
| [Llama3.3](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/llama) | meta-llama/Llama-3.3-70B-Instruct                            |
| [Baichuan](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/baichuan) | baichuan-inc/Baichuan-7B, baichuan-inc/Baichuan-13B-Base, baichuan-inc/Baichuan-13B-Chat |
| [Baichuan2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/baichuan) | baichuan-inc/Baichuan2-7B-Base, baichuan-inc/Baichuan2-7B-Chat, baichuan-inc/Baichuan2-13B-Base, baichuan-inc/Baichuan2-13B-Chat |
| [Bloom](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/bloom) | bigscience/bloom-560m, bigscience/bloom-560m-bf16, bigscience/bloom-1b1, bigscience/bloom-3b, bigscience/bloom-7b1, bigscience/bloomz-560m, bigscience/bloomz-1b1, bigscience/bloomz-3b, bigscience/bloomz-7b1-mt, bigscience/bloomz-7b1-p3, bigscience/bloomz-7b1, bellegroup/belle-7b-2m |
| [ChatGLM](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/chatglm/) | THUDM/chatglm-6b, THUDM/chatglm-6b-v1.1                      |
| [ChatGLM2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/chatglm2) | THUDM/chatglm2-6b                                            |
| [ChatGLM3](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/chatglm2) | THUDM/chatglm3-6b                                            |
| [DeepSeekV2](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/config/deepseek-v2) | deepseek-ai/DeepSeek-V2, deepseek-ai/DeepSeek-V2-Chat, deepseek-ai/DeepSeek-V2-Lite, deepseek-ai/DeepSeek-V2-Lite-Chat, deepseek-ai/DeepSeek-Coder-V2-Base, deepseek-ai/DeepSeek-Coder-V2-Instruct, deepseek-ai/DeepSeek-Coder-V2-Lite-Base, deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct |
| [DeepSeekV3](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/config/deepseek-v2) | deepseek-ai/DeepSeek-V3, deepseek-ai/DeepSeek-V3-Base        |
| [DeepSeek-R1](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/config/deepseek-v2) | deepseek-ai/DeepSeek-R1, deepseek-ai/DeepSeek-R1-Zero, deepseek-ai/DeepSeek-R1-Distill-Llama-70B, deepseek-ai/DeepSeek-R1-Distill-Llama-8B, deepseek-ai/DeepSeek-R1-Distill-Qwen-14B, deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, deepseek-ai/DeepSeek-R1-Distill-Qwen-7B |
| [Gemma](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/gemma) | google/gemma-7b, google/gemma-7b-it, google/gemma-2b, google/gemma-2b-it |
| [Mistral](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/mistral) | mistralai/Mistral-7B-Instruct-v0.3, mistralai/Mistral-7B-v0.1 |
| [Mixtral](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/mixtral) | mistralai/Mixtral-8x7B-Instruct-v0.1                         |
| [OPT](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/opt) | facebook/opt-125m, facebook/opt-350m, facebook/opt-1.3b, facebook/opt-2.7b, facebook/opt-6.7b, facebook/opt-13b, facebook/opt-30b, facebook/opt-66b, facebook/opt-iml-1.3b, opt-iml-max-1.3b |
| [Qwen](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | qwen/qwen-7b, qwen/qwen-7b-chat, qwen/qwen-14b, qwen/qwen-14b-chat, qwen/qwen-72b, qwen/qwen-72b-chat, |
| [Qwen1.5](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/Qwen1.5-0.5B, Qwen/Qwen1.5-0.5B-Chat, Qwen/Qwen1.5-1.8B, Qwen/Qwen1.5-1.8B-Chat, Qwen/Qwen1.5-4B, Qwen/Qwen1.5-4B-Chat, Qwen/Qwen1.5-7B, Qwen/Qwen1.5-7B-Chat, Qwen/Qwen1.5-14B, Qwen/Qwen1.5-14B-Chat, Qwen/Qwen1.5-32B, Qwen/Qwen1.5-32B-Chat, Qwen/Qwen1.5-72B, Qwen/Qwen1.5-72B-Chat, Qwen/Qwen1.5-110B, Qwen/Qwen1.5-110B-Chat, Qwen/Qwen1.5-MoE-A2.7B, Qwen/Qwen1.5-MoE-A2.7B-Chat |
| [Qwen2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/Qwen2-0.5B, Qwen/Qwen2-0.5B-Instruct, Qwen/Qwen2-1.5B, Qwen/Qwen2-1.5B-Instruct, Qwen/Qwen2-7B, Qwen/Qwen2-7B-Instruct, Qwen/Qwen2-72B, Qwen/Qwen2-72B-Instruct, Qwen/Qwen2-57B-A14B, Qwen/Qwen2-57B-A14B-Instruct |
| [Qwen2-Math](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/Qwen2-Math-1.5B, Qwen/Qwen2-Math-1.5B-Instruct, Qwen/Qwen2-Math-7B, Qwen/Qwen2-Math-7B-Instruct, Qwen/Qwen2-Math-72B, Qwen/Qwen2-Math-72B-Instruct, Qwen/Qwen2-Math-RM-72B |
| [Qwen2.5](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/Qwen2.5-0.5B, Qwen/Qwen2.5-0.5B-Instruct, Qwen/Qwen2.5-1.5B, Qwen/Qwen2.5-1.5B-Instruct, Qwen/Qwen2.5-3B, Qwen/Qwen2.5-3B-Instruct, Qwen/Qwen2.5-7B, Qwen/Qwen2.5-7B-Instruct, Qwen/Qwen2.5-7B-Instruct-1M, Qwen/Qwen2.5-14B, Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-14B-Instruct-1M, Qwen/Qwen2.5-32B, Qwen/Qwen2.5-32B-Instruct, Qwen/Qwen2.5-72B, Qwen/Qwen2.5-72B-Instruct |
| [Qwen2.5-Math](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/Qwen2.5-Math-1.5B, Qwen/Qwen2.5-Math-1.5B-Instruct, Qwen/Qwen2.5-Math-7B, Qwen/Qwen2.5-Math-7B-Instruct, Qwen/Qwen2.5-Math-72B, Qwen/Qwen2.5-Math-72B-Instruct, Qwen/Qwen2.5-Math-RM-72B |
| [Qwen2.5-Coder](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/Qwen2.5-Coder-1.5B, Qwen/Qwen2.5-Coder-1.5B-Instruct, Qwen/Qwen2.5-Coder-7B, Qwen/Qwen2.5-Coder-7B-Instruct |
| [QwQ](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/qwen/) | Qwen/QwQ-32B, Qwen/QwQ-32B-Preview                           |
| [Yuan2](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm/config/yuan/) | IEITYuan/Yuan2-2B, IEITYuan/Yuan2-51B, IEITYuan/Yuan2-102B   |



## æ¨¡å‹é¢„è®­ç»ƒæ€§èƒ½

ä»¥ä¸‹æµ‹è¯•ç»“æœåŸºäºä»¥ä¸‹æœºå™¨ç¯å¢ƒï¼š

- **GPU**: A100 80G * 8, CUDA 11.8, NCCL 2.15
- **CPU**: Intel(R) Xeon(R) Platinum 8350C CPU @ 2.60GHz
- **å†…å­˜**: 1 TB

```bash
paddle commit id: 9b36e53f24ac5f471b20de99e0cc3980f38b44ab
paddlenlp commit id: 0b246a609a3062e3c3256d87193b70277b5b07e0
```



## æ¨¡å‹æ€§èƒ½æµ‹è¯•æ±‡æ€»

| æ¨¡å‹                              | åºåˆ—é•¿åº¦ | åˆ†å¸ƒå¼ç­–ç•¥    | é€Ÿåº¦ (tokens/card/sec) | æ˜¾å­˜å ç”¨ (MB) | é…ç½®æ–‡ä»¶                                                 | æµ‹è¯•æ—¶é—´            |
| --------------------------------- | -------- | ------------- | ---------------------- | ------------- | -------------------------------------------------------- | ------------------- |
| FlagAlpha/Llama2-Chinese-13b-Chat | 4096     | tp2sd4_stage2 | 1980.22                | 64323         | ./llama/pretrain-flagalpha_llama2_13b-tp2sd4_stage2.json | 2023-11-27 21:42:38 |
| FlagAlpha/Llama2-Chinese-7b-Chat  | 4096     | tp2sd4_stage2 | 3744.62                | 52092         | ./llama/pretrain-flagalpha_llama2_7b-tp2sd4_stage2.json  | 2023-11-27 21:44:57 |
| baichuan-inc/Baichuan2-13B-Base   | 4096     | sd8_stage2    | 1354.99                | 74767         | ./baichuan/pretrain-baichuan2_13b-sd8_stage2.json        | 2023-11-27 21:51:26 |
| baichuan-inc/Baichuan2-7B-Base    | 4096     | tp2sd4_stage2 | 3542.45                | 58363         | ./baichuan/pretrain-baichuan2_7b-tp2sd4_stage2.json      | 2023-11-27 21:53:58 |
| facebook/llama-13b                | 4096     | tp2sd4_stage2 | 1969.64                | 64278         | ./llama/pretrain-llama_13b-tp2sd4_stage2.json            | 2023-11-27 21:58:03 |
| facebook/llama-7b                 | 4096     | tp2sd4_stage2 | 3754.73                | 52092         | ./llama/pretrain-llama_7b-tp2sd4_stage2.json             | 2023-11-27 22:00:30 |
| idea-ccnl/ziya-llama-13b-v1       | 4096     | tp2sd4_stage2 | 1968.34                | 63983         | ./llama/pretrain-ziya_llama_13b-tp2sd4_stage2.json       | 2023-11-27 22:04:35 |
| linly-ai/chinese-llama-2-7b       | 4096     | tp2sd4_stage2 | 3732.9                 | 51751         | ./llama/pretrain-linly_llama2_7b-tp2sd4_stage2.json      | 2023-11-27 22:06:58 |
| meta-llama/Llama-2-13b            | 4096     | tp2sd4_stage2 | 1975.63                | 64294         | ./llama/pretrain-llama2_13b-tp2sd4_stage2.json           | 2023-11-27 22:11:04 |
| meta-llama/Llama-2-7b             | 4096     | tp2sd4_stage2 | 3755.21                | 52092         | ./llama/pretrain-llama2_7b-tp2sd4_stage2.json            | 2023-11-27 22:13:34 |
| qwen/qwen-7b                      | 4096     | tp2sd4_stage2 | 3607.28                | 65448         | ./qwen/pretrain-qwen_7b-tp2sd4_stage2.json               | 2023-11-27 22:16:04 |

**è¯´æ˜**

- **é€Ÿåº¦å•ä½**: `tokens/card/sec`ï¼Œè¡¨ç¤ºæ¯å¼ å¡æ¯ç§’éœ€è®­ç»ƒçš„ token æ•°ã€‚
- **é€Ÿåº¦æ³¢åŠ¨**: é€Ÿåº¦ä¼šæœ‰å°å¹…æ³¢åŠ¨ï¼Œä¾‹å¦‚ `facebook/llama-7b` å’Œ `meta-llama/Llama-2-7b` æ˜¯ç›¸åŒè®­ç»ƒé…ç½®ã€‚
- **æ˜¾å­˜å ç”¨**: æ˜¾å­˜å ç”¨å•ä½æ˜¯ MBï¼Œä½¿ç”¨çš„æ˜¯ `max_memory_allocated` è·å–æ˜¾å­˜ï¼Œå®é™…ç‰©ç†æ˜¾å­˜ä¼šå ç”¨æ›´å¤šï¼Œå¤§çº¦å¤š 2-3GBã€‚
