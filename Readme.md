# AI Multi-Persona Chatbot & Agent Framework

This project is a **modular AI chatbot and agent framework** designed to demonstrate best practices for building **LLM-powered systems** with personas, guardrails, memory management, structured outputs, tool usage, and evaluation.

It is intended for **learning, experimentation, and prototyping** rather than production use.

---

## âœ¨ Key Features

- **Multi-persona routing**
  - Automatically routes user input to `tutor`, `support`, or `other` personas.
- **Agent loop with tool calling**
  - Supports safe tool execution (e.g. calculator).
- **Strict JSON schema enforcement**
  - Ensures predictable and machine-readable LLM outputs.
- **Conversation memory & summarization**
  - Compresses long conversations into summaries.
- **Confidence & quality evaluation**
  - Evaluates answers using a secondary LLM evaluator.
- **Prompt-injection guardrails**
  - Blocks common malicious prompt patterns.
- **Retry & validation logic**
  - Automatically retries invalid LLM responses.

---

## ğŸ§  Architecture Overview

```
User Input
   â†“
Guardrails (guardrails.py)
   â†“
Persona Router (router.py)
   â†“
Conversation Manager (chatbot.py)
   â†“
Agent Loop
   â”œâ”€ LLM Call (llm.py)
   â”œâ”€ JSON Validation (Json_structure.py)
   â”œâ”€ Tool Execution (tools.py)
   â””â”€ Evaluation (check.py)
   â†“
Structured Response
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ chatbot.py          # Main conversation + agent loop
â”œâ”€â”€ router.py           # Persona routing logic
â”œâ”€â”€ persona.py          # Persona system prompts
â”œâ”€â”€ llm.py              # LLM interface & retry logic
â”œâ”€â”€ Json_structure.py   # Output schema & JSON validation
â”œâ”€â”€ tools.py            # Tool registry and execution
â”œâ”€â”€ check.py            # Confidence checks & evaluation
â”œâ”€â”€ summary.py          # Conversation summarization
â”œâ”€â”€ guardrails.py       # Prompt-injection protection
â”œâ”€â”€ run.py              # Example runner
â””â”€â”€ README.md
```

---

## ğŸ¤– Personas

| Persona  | Description |
|--------|-------------|
| Tutor  | Explains NLP & LLM concepts step-by-step |
| Support | Handles product and technical issues |
| Other  | Fallback for out-of-scope queries |

---

## ğŸ”§ Tool System

Currently supported tools:

- **calculator**
  - Evaluates mathematical expressions safely
  - Example:
    ```json
    {
      "tool": "calculator",
      "arguments": { "expression": "23 * 17" }
    }
    ```

The agent loop automatically:
1. Detects tool requests
2. Validates arguments
3. Executes the tool
4. Feeds the result back to the LLM

---

## ğŸ“Š Evaluation & Confidence

Each response includes:
- `confidence` (0â€“1)
- `evaluation.score` (LLM-based quality judgment)
- `warnings` for:
  - Low confidence
  - Short or empty answers
  - Low evaluation scores

Decision logic determines whether to:
- Accept the response
- Warn the user
- Flag low confidence

---

## ğŸ›¡ï¸ Safety & Guardrails

- Blocks common prompt-injection attempts
- Prevents memory poisoning
- Restricts tool execution
- Enforces strict JSON-only responses

---

## â–¶ï¸ How to Run

```bash
python run.py
```

Example:
```python
from chatbot import chat

response = chat("What is tokenization in NLP?")
print(response)
```

Structured JSON mode:
```python
response = chat_json("What is 23 * 17?")
```

---

## âš ï¸ Known Limitations

- LLM backend is mocked (no real API connected)
- Some validation logic contains intentional bugs for practice
- Tool execution error handling is minimal
- Not production-hardened

---

## ğŸ¯ Purpose

This project is ideal for:
- Learning **LLM system design**
- Practicing **agent architectures**
- Understanding **tool calling**
- Debugging **structured AI pipelines**
- Interview & portfolio demonstrations

---

## ğŸ“œ License

Educational / experimental use only.
