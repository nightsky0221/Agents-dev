#include "xpu/kernel/cluster.h"
#include "xpu/kernel/cluster_partition.h"
#include "xpu/kernel/cluster_primitive.h"

namespace xpu3 {
namespace plugin {

static inline __device__ void batch_load(
        _global_ptr_ const int32_t* kv_seq_lod,
        _global_ptr_ const int32_t* start_token,
        _global_ptr_ const int32_t* real_batch,
        int* kv_seq_lod_lm,
        int* start_token_lm,
        int* real_batch_lm,
        int batch_begin,
        int read_batch_len) {
    GM2LM_ASYNC(kv_seq_lod + batch_begin, kv_seq_lod_lm, (read_batch_len + 1) * sizeof(int));
    GM2LM_ASYNC(start_token + batch_begin, start_token_lm, read_batch_len * sizeof(int));
    GM2LM_ASYNC(real_batch + batch_begin, real_batch_lm, read_batch_len * sizeof(int));
    mfence();
}

template <typename TI, typename TO, typename TID>
__global__ void absorb_mla_cache_write(
        const TI* src,
        TO* dst,
        const TID* block_table,
        int32_t* kv_seq_lod,
        int32_t* start_token,
        int32_t* real_batch,
        int64_t batch_size,
        int64_t kv_head_num,
        int64_t head_dim,
        int64_t max_seq_num,
        int64_t block_size,
        int64_t max_num_blocks_per_seq) {
    constexpr int loop_batch_size = 32;
    int cid = core_id();
    int ncores = core_num();
    int tid = cid * cluster_num() + cluster_id();
    int nthreads = ncores * cluster_num();
    // prepare lm buffer
    __local__ TI local_scr[2048];
    __local__ int lod_lm[loop_batch_size];
    __local__ int kv_seq_lod_lm[loop_batch_size];
    __local__ int start_token_lm[loop_batch_size];
    __local__ int real_batch_lm[loop_batch_size];
    int table_cnt = max_seq_num * max_num_blocks_per_seq;
    batch_load(
            kv_seq_lod,
            start_token,
            real_batch,
            kv_seq_lod_lm,
            start_token_lm,
            real_batch_lm,
            0,
            batch_size);    
    int token_num = kv_seq_lod[batch_size];
    for(int i = tid; i < token_num; i += nthreads){
        int batch = 0;
        for(int b = 0; b < batch_size; b++){
            if(i < lod_lm[b + 1]){
                batch = b;
                break;
            }
            GM2LM(src + i * kv_head_num * head_dim, local_scr, kv_head_num * head_dim * sizeof(TI));
            TID block_id = block_table[real_batch_lm[batch] * max_num_blocks_per_seq + (start_token_lm[batch] + i - lod_lm[b]) / block_size];
            LM2GM(local_scr, dst + block_id * kv_head_num * block_size * head_dim + (start_token_lm[batch] + i - lod_lm[b]) % block_size * head_dim, kv_head_num * head_dim * sizeof(TO));

        }
    }
}




#define _XPU_DEF__ABSORB_MLA_CACHE_WRITE(TI, TO, TID)             \
    template __global__ void absorb_mla_cache_write<TI, TO, TID>( \
            const TI*,                                                                     \
            TO*,                                                                           \
            const TID*,                                                                    \
            int32_t*,                                                                      \
            int32_t*,                                                                      \
            int32_t*,                                                                      \
            int64_t,                                                                       \
            int64_t,                                                                       \
            int64_t,                                                                       \
            int64_t,                                                                       \
            int64_t,                                                                       \
            int64_t);

_XPU_DEF__ABSORB_MLA_CACHE_WRITE(float16, float16, int32_t);
_XPU_DEF__ABSORB_MLA_CACHE_WRITE(bfloat16, bfloat16, int32_t);

}  // namespace plugin
}  // namespace xpu3
