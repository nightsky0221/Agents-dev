// Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "xpu/kernel/cluster.h"
#include "xpu/kernel/cluster_partition.h"
#include "xpu/kernel/cluster_primitive.h"
#include "xpu/kernel/cluster_debug.h"

namespace xpu3 {
namespace plugin {

static __device__ void apply_rotary_pos_emb_nonexo(
        float* qk,
        float* cos_sin,
        int rot_dim,
        int head_size,
        int element_num) {
    const int emb_dim = rot_dim / 2;
    for(int i = 0; i < emb_dim; i++){
        float cos_ind = cos_sin[i];
        float sin_ind = cos_sin[i + emb_dim];
        for (int j = 0; j < element_num; j++) {
            // if(i == 0){
            //     printf("input %f %f   %f %f  ",qk[j * head_size],qk[j * head_size + 1],cos_ind , sin_ind);
            // }
            float x_ind = qk[j * head_size + i * 2];
            float y_ind = qk[j * head_size + i * 2 + 1];
            qk[j * head_size + i * 2] = x_ind * cos_ind - y_ind * sin_ind;
            qk[j * head_size + i * 2 + 1] =  y_ind * cos_ind + x_ind * sin_ind;
            // if(i == 0){
            //     printf("output %f %f \n ",qk[j * head_size],qk[j * head_size + 1]);
            // }
        }   
    }

    mfence();
}


static __device__ void apply_rotary_pos_emb_nexo(
        float* qk,
        float* cos_sin,
        int rot_dim,
        int head_size,
        int element_num) {
    float32x16_t vqx_0;
    float32x16_t vqx_1;
    float32x16_t vqy_0;
    float32x16_t vqy_1;
    float32x16_t vcos_0;
    float32x16_t vcos_1;
    float32x16_t vsin_0;
    float32x16_t vsin_1;
    float32x16_t v_x_cos;
    float32x16_t v_y_cos;
    const float var = -1.0f;
    const int emb_dim = rot_dim / 2;
    // TODO: fixme. It assumes that rot_dim is the multiplier of 32.
    for (int i = 0; i < emb_dim; i += 32) {
        vload2_lm(cos_sin + i, vcos_0, vcos_1);
        vload2_lm(cos_sin + i + emb_dim, vsin_0, vsin_1);
        for (int j = 0; j < element_num; j++) {
            int offset = j * head_size;
            vload2_lm(qk + offset + i, vqx_0, vqx_1);
            vload2_lm(qk + offset + i + emb_dim, vqy_0, vqy_1);
            // qx
            v_x_cos = vvmul_float32x16(vqx_0, vcos_0);
            vqx_0 = vvmul_float32x16(vqx_0, vsin_0);
            // qy * cons
            v_y_cos = vvmul_float32x16(vqy_0, vcos_0);
            // -qy
            vqy_0 = svmul_float32x16(var, vqy_0);
            // -qy * sin
            vqy_0 = vvmul_float32x16(vqy_0, vsin_0);
            // add
            vqy_0 = vvadd_float32x16(v_x_cos, vqy_0);  // out_left
            vqx_0 = vvadd_float32x16(vqx_0, v_y_cos);  // out_right
            // qy
            v_x_cos = vvmul_float32x16(vqx_1, vcos_1);
            vqx_1 = vvmul_float32x16(vqx_1, vsin_1);
            // qy * cons
            v_y_cos = vvmul_float32x16(vqy_1, vcos_1);
            // -qy
            vqy_1 = svmul_float32x16(var, vqy_1);
            // -qy * sin
            vqy_1 = vvmul_float32x16(vqy_1, vsin_1);
            // add
            vqy_1 = vvadd_float32x16(v_x_cos, vqy_1);  // out_left
            vqx_1 = vvadd_float32x16(vqx_1, v_y_cos);  // out_right
            vstore2_lm(qk + offset + i, vqy_0, vqy_1);
            vstore2_lm(qk + offset + i + emb_dim, vqx_0, vqx_1);
            mfence();
        }
    }
}

template <typename TX, typename TY>
static __device__ void fast_cast(TX* x, TY* y, int64_t n) {
    primitive_cast<TX, TY>(x, y, n);
}

template <>
__device__ void fast_cast<bfloat16, float>(bfloat16* x, float* y, int64_t n) {
    int start = (n - 1) / 32 * 32;
    x = x + start;
    y = y + start;
    float32x16_t xl;
    float32x16_t xh;
    for (int i = start; i >= 0; i -= 32) {
        vload2_lm(x, xl, xh);
        vstore2_lm(y, xl, xh);
        x -= 32;
        y -= 32;
    }
    mfence_lm();
}
template <>
__device__ void fast_cast<float, bfloat16>(float* x, bfloat16* y, int64_t n) {
    float32x16_t xl;
    float32x16_t xh;
    for (int i = 0; i < n; i += 32) {
        vload2_lm(x, xl, xh);
        vstore2_lm(y, xl, xh);
        x += 32;
        y += 32;
    }
    mfence_lm();
}

template <typename T, typename TR>
__global__ void rotary_embedding_neox(
        const int* positions,  // [num_tokens]
        T* query,                  // [num_tokens, num_heads, head_size]
        T* key,                    // [num_tokens, num_kv_heads, head_size]
        const TR* cos_sin_cache,   // [max_position, 2, rot_dim // 2]
        const int rot_dim,
        const int num_heads,
        const int num_kv_heads,
        const int head_size,
        const int32_t num_tokens) {
    int cid = core_id();
    int ncores = core_num();
    if (cid >= ncores) {
        return;
    }
    int tid = cluster_id() * ncores + cid;
    int nthreads = ncores * cluster_num();
    constexpr int sm_buffer_size = 64 * 512;
    int head_num = 2048 / sizeof(T) / head_size;  // max DMA is 2048 Bytes
    int lm_buffer_size = head_num * head_size;
    __simd__ float data_lm[lm_buffer_size];
    __simd__ float cos_sin_lm[rot_dim];
    __simd__ __shared__ int pos_sm[sm_buffer_size];

    int32_t cluster_start_idx = 0;
    int32_t cluster_end_idx = 0;
    int32_t start_idx = 0;
    int32_t end_idx = 0;
    partition(cluster_id(), cluster_num(), num_tokens, 1, &cluster_start_idx, &cluster_end_idx);
    if(cid == 0 && cluster_end_idx - cluster_start_idx > 0){
        GM2SM(positions + cluster_start_idx, pos_sm, (cluster_end_idx - cluster_start_idx) * sizeof(int));
    }
    partition(cid, ncores, cluster_end_idx - cluster_start_idx, 1, &start_idx, &end_idx);
    start_idx = start_idx + cluster_start_idx;
    end_idx = end_idx + cluster_start_idx;
    mfence();
    sync_all();
    // printf("id %d %d  %d %d\n ",cluster_id(), cluster_start_idx, cluster_end_idx, pos_sm[0]);
    for (int64_t token_idx = start_idx; token_idx < end_idx; token_idx += 1) {
        int64_t pos = pos_sm[token_idx - cluster_start_idx];
        // printf("poss %lld %lld \n ",token_idx, pos);

        _global_ptr_ const TR* cache_ptr = cos_sin_cache + pos * rot_dim;
        int64_t offset_q = token_idx * num_heads * head_size;
        int64_t offset_k = token_idx * num_kv_heads * head_size;
        // get cos & sin
        if (num_tokens > token_idx) {
            GM2LM(cache_ptr, cos_sin_lm, rot_dim * sizeof(TR));
            fast_cast<TR, float>((TR*)cos_sin_lm, cos_sin_lm, rot_dim);
        }
        // generate query
        for (int64_t i = 0; i < num_heads; i += head_num) {
            // int loop_size = min<int64_t>(n - nstart, lm_buffer_size);
            int64_t element_num = min<int64_t>(num_heads - i, head_num);
            GM2LM(query + offset_q, data_lm, element_num * head_size * sizeof(T));
            fast_cast<T, float>((T*)data_lm, data_lm, element_num * head_size);
            // calculation
            apply_rotary_pos_emb_nonexo(data_lm, cos_sin_lm, rot_dim, head_size, element_num);
            // write back to GM
            fast_cast<float, T>(data_lm, (T*)data_lm, element_num * head_size);
            LM2GM(data_lm, query + offset_q, element_num * head_size * sizeof(T));
            offset_q += (head_size * element_num);
        }
        // generate key
        for (int64_t j = 0; j < num_kv_heads; j += head_num) {
            int64_t element_num = min<int64_t>(num_kv_heads - j, head_num);
            GM2LM(key + offset_k, data_lm, element_num * head_size * sizeof(T));
            fast_cast<T, float>((T*)data_lm, data_lm, element_num * head_size);
            // calculation
            apply_rotary_pos_emb_nonexo(data_lm, cos_sin_lm, rot_dim, head_size, element_num);
            // write back to GM
            fast_cast<float, T>(data_lm, (T*)data_lm, element_num * head_size);
            LM2GM(data_lm, key + offset_k, element_num * head_size * sizeof(T));
            offset_k += (head_size * element_num);
        }
    }
}

#define _XPU_DEF_(T, TR)                                        \
    template __global__ void rotary_embedding_neox<T, TR>( \
            const int* positions,                           \
            T* query,                                           \
            T* key,                                             \
            const TR* cos_sin_cache,                            \
            const int rot_dim,                                  \
            const int num_heads,                                \
            const int num_kv_heads,                             \
            const int head_size,                                \
            const int32_t num_tokens);

_XPU_DEF_(float, float);
_XPU_DEF_(float16, float16);
_XPU_DEF_(bfloat16, float);
_XPU_DEF_(bfloat16, bfloat16);

}
}
